import{_ as s,H as d,W as r,X as i,Y as e,Z as t,$ as o,a0 as l}from"./framework-cd0f4961.js";const p={},c={href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html",target:"_blank",rel:"noopener noreferrer"};function h(m,a){const n=d("ExternalLinkIcon");return r(),i("div",null,[a[2]||(a[2]=e("h1",{id:"如何使用",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#如何使用","aria-hidden":"true"},"#"),t(" 如何使用")],-1)),a[3]||(a[3]=e("h2",{id:"前提条件",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#前提条件","aria-hidden":"true"},"#"),t(" 前提条件")],-1)),e("p",null,[a[1]||(a[1]=t("install nvidia-container-toolkit, see: ")),e("a",c,[a[0]||(a[0]=t("https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html")),o(n)])]),a[4]||(a[4]=l(`<h2 id="启动服务" tabindex="-1"><a class="header-anchor" href="#启动服务" aria-hidden="true">#</a> 启动服务</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./start-service.sh <span class="token operator">&lt;</span>config-name<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="请求服务" tabindex="-1"><a class="header-anchor" href="#请求服务" aria-hidden="true">#</a> 请求服务</h2><h3 id="openai-text-api" tabindex="-1"><a class="header-anchor" href="#openai-text-api" aria-hidden="true">#</a> openai-text api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-openai-text.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="ollama-text-api" tabindex="-1"><a class="header-anchor" href="#ollama-text-api" aria-hidden="true">#</a> ollama-text api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-ollama-text.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="openai-image-api" tabindex="-1"><a class="header-anchor" href="#openai-image-api" aria-hidden="true">#</a> openai-image api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-openai-image.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="停止服务" tabindex="-1"><a class="header-anchor" href="#停止服务" aria-hidden="true">#</a> 停止服务</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./stop-service.sh <span class="token operator">&lt;</span>config-name<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="table-config-name-model-api-protocol" tabindex="-1"><a class="header-anchor" href="#table-config-name-model-api-protocol" aria-hidden="true">#</a> Table: config-name -&gt; model &amp; api-protocol</h2><table><thead><tr><th>config-name</th><th>model</th><th>api-protocol</th></tr></thead><tbody><tr><td>llama3-8b</td><td>Llama3-8B</td><td>openai-text</td></tr><tr><td>llama3-70b_1</td><td>Llama3-70B</td><td>openai-text</td></tr><tr><td>llama3-70b_4</td><td>Llama3-70B</td><td>openai-text</td></tr><tr><td>qwen1.5-110b_1</td><td>Qwen1.5-110B</td><td>openai-text</td></tr><tr><td>qwen1.5-110b_4</td><td>Qwen1.5-110B</td><td>openai-text</td></tr><tr><td>yi1.5-34b</td><td>Yi1.5-34B</td><td>openai-text</td></tr><tr><td>qwen2-72b_1</td><td>Qwen2-72B</td><td>openai-text</td></tr><tr><td>qwen2-72b_4</td><td>Qwen2-72B</td><td>openai-text</td></tr><tr><td>falcon2-11b</td><td>Falcon2-11B</td><td>openai-text</td></tr><tr><td>openbiollm-llama3-70b_1</td><td>OpenBioLLM-Llama3-70B</td><td>openai-text</td></tr><tr><td>openbiollm-llama3-70b_4</td><td>OpenBioLLM-Llama3-70B</td><td>openai-text</td></tr><tr><td>minicpm-v2.5</td><td>MiniCPM-Llama3-V2.5</td><td>ollama-text</td></tr><tr><td>codestral-22b-v0.1</td><td>Codestral-22B-v0.1</td><td>openai-text</td></tr><tr><td>gemma2-27b_4</td><td>Gemma-2-27B</td><td>openai-text</td></tr><tr><td>llama3.1-405b_8</td><td>Llama-3.1-405B</td><td>openai-text</td></tr><tr><td>mistral-123b_4</td><td>Mistral-123B</td><td>openai-text</td></tr><tr><td>flux.1-dev</td><td>FLUX.1-dev</td><td>openai-image</td></tr><tr><td>nemotron-70b_2</td><td>Llama-3.1-Nemotron-70B</td><td>openai-text</td></tr><tr><td>nvlm-70b_4</td><td>NVLM-D-72B</td><td>openai-text</td></tr></tbody></table><h1 id="如何构建docker镜像" tabindex="-1"><a class="header-anchor" href="#如何构建docker镜像" aria-hidden="true">#</a> 如何构建docker镜像</h1><p>下面是关于如何构建一个模型服务镜像的概述：</p><h2 id="_1-明确交付需求" tabindex="-1"><a class="header-anchor" href="#_1-明确交付需求" aria-hidden="true">#</a> 1. 明确交付需求</h2><p>在构建镜像之前，首先需要明确交付需求，以确保部署镜像与需求目标的一致性。以下是一些关键要素：</p><ul><li><p><strong>模型选择</strong>：确定需要部署的模型类型，如是文生文、文生图、还是多模态等。确定具体模型，如 Llama3-8B、Qwen1.5-110B 或 Falcon2-11B。</p></li><li><p><strong>部署资源</strong>：明确将用于部署的硬件资源，特别是是否需要 GPU 加速，以及选择的 GPU 类型（如 NVIDIA A100、H100、RTX4090等）以及数量。</p></li><li><p><strong>部署方式</strong>：确定部署的方式，是本地部署还是云端部署，模型文件是否需要打包进镜像等。</p></li><li><p><strong>服务接口协议</strong>：选择合适的 API 协议，确保模型服务可以通过标准化的方式被访问。不同类型的模型服务有不同的协议可供选择。如文生文常见的协议包括 openai协议，ollama协议，hf的tgi协议 等。</p></li><li><p><strong>性能参数</strong>：明确模型的一些性能目标，如吞吐量、响应时间、扩展性要求等。</p></li></ul><h2 id="_2-找到最符合需求的开源工具-框架" tabindex="-1"><a class="header-anchor" href="#_2-找到最符合需求的开源工具-框架" aria-hidden="true">#</a> 2. 找到最符合需求的开源工具/框架</h2><p>根据模型的类型和部署环境，选择合适的开源工具和框架以简化构建工作。根据不同类型的任务，常见的工具框架包括：</p><ul><li><p><strong>文生文</strong>：lmdeploy，ollama，vllm，localai，tgi</p></li><li><p><strong>文生图</strong>：comfyui, stable-diffusion-webui, localai</p></li></ul><h2 id="_3-开发调试" tabindex="-1"><a class="header-anchor" href="#_3-开发调试" aria-hidden="true">#</a> 3. 开发调试</h2><p>在构建镜像之前，需要进行开发调试，确保模型能够在容器中正常运行。开发调试的步骤包括：</p><ul><li><p><strong>调整部署参数</strong>：根据任务需求，模型参数等，调整docker compose file中的运行参数，环境变量等，确保模型的正确运行。</p></li><li><p><strong>模型适配</strong>：确保模型可以在容器环境中无缝运行，处理依赖项的安装和配置问题。某些模型可能需要特定的库、驱动程序或环境变量设置等。在 Dockerfile 中准确地定义这些依赖。某些模型可能引入了新的模型结构，需进一步对代码进行定制化开发实现适配。</p></li><li><p><strong>解决 bug</strong>：根据测试镜像运行时的报错进行排查，解决开源项目中可能存在的bug。</p></li><li><p><strong>优化代码实现</strong>：针对模型测试运行中发现的一些性能问题，优化代码实现如资源分配，模型调度等逻辑，确保合理的运行性能。</p></li></ul><h2 id="_4-镜像打包" tabindex="-1"><a class="header-anchor" href="#_4-镜像打包" aria-hidden="true">#</a> 4. 镜像打包</h2><p>完成开发和调试后，可以开始打包 Docker 镜像。以下是具体步骤：</p><ol><li><p><strong>编写 Dockerfile</strong>：根据不同情况，Dockerfile的编写有如下可能的方式：</p><ul><li>基于开源项目Dockerfile，修改后重新从头构建</li><li>基于开源镜像作为基础镜像，加入修改的内容后进行增量构建</li><li>自行编写Dockerfile，从头构建一个新的定制化镜像</li></ul></li><li><p><strong>构建镜像</strong>：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> build <span class="token parameter variable">-t</span> <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span> <span class="token builtin class-name">.</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><p><strong>测试镜像</strong>：使用 GPU 资源启动容器并进行测试，确保镜像中的模型服务可以正常响应请求。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-d</span> <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><p><strong>推送到镜像仓库</strong>：如果要在多节点或云端环境中部署，可以将镜像推送到 Docker Hub、AWS ECR 等容器镜像仓库：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> push <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ol><p>通过以上步骤，你可以顺利完成一个用于模型服务的 Docker 镜像构建，确保模型能够正确运行并满足需求。</p>`,28))])}const g=s(p,[["render",h],["__file","docker_build.html.vue"]]);export{g as default};
