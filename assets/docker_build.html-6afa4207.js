import{_ as s,H as d,W as r,X as i,Y as e,Z as t,$ as o,a0 as l}from"./framework-0c0bf18e.js";const p={},c={href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html",target:"_blank",rel:"noopener noreferrer"};function h(m,a){const n=d("ExternalLinkIcon");return r(),i("div",null,[a[2]||(a[2]=e("h1",{id:"사용방법",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#사용방법","aria-hidden":"true"},"#"),t(" 사용방법")],-1)),a[3]||(a[3]=e("h2",{id:"전제조건",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#전제조건","aria-hidden":"true"},"#"),t(" 전제조건")],-1)),e("p",null,[a[1]||(a[1]=t("install nvidia-container-toolkit, see: ")),e("a",c,[a[0]||(a[0]=t("https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html")),o(n)])]),a[4]||(a[4]=l(`<h2 id="서비스-시작" tabindex="-1"><a class="header-anchor" href="#서비스-시작" aria-hidden="true">#</a> 서비스 시작</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./start-service.sh <span class="token operator">&lt;</span>config-name<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="서비스-요청" tabindex="-1"><a class="header-anchor" href="#서비스-요청" aria-hidden="true">#</a> 서비스 요청</h2><h3 id="openai-text-api" tabindex="-1"><a class="header-anchor" href="#openai-text-api" aria-hidden="true">#</a> openai-text api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-openai-text.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="ollama-text-api" tabindex="-1"><a class="header-anchor" href="#ollama-text-api" aria-hidden="true">#</a> ollama-text api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-ollama-text.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="openai-image-api" tabindex="-1"><a class="header-anchor" href="#openai-image-api" aria-hidden="true">#</a> openai-image api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-openai-image.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="서비스-중지" tabindex="-1"><a class="header-anchor" href="#서비스-중지" aria-hidden="true">#</a> 서비스 중지</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./stop-service.sh <span class="token operator">&lt;</span>config-name<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="table-config-name-model-api-protocol" tabindex="-1"><a class="header-anchor" href="#table-config-name-model-api-protocol" aria-hidden="true">#</a> Table: config-name -&gt; model &amp; api-protocol</h2><table><thead><tr><th>config-name</th><th>model</th><th>api-protocol</th></tr></thead><tbody><tr><td>llama3-8b</td><td>Llama3-8B</td><td>openai-text</td></tr><tr><td>llama3-70b_1</td><td>Llama3-70B</td><td>openai-text</td></tr><tr><td>llama3-70b_4</td><td>Llama3-70B</td><td>openai-text</td></tr><tr><td>qwen1.5-110b_1</td><td>Qwen1.5-110B</td><td>openai-text</td></tr><tr><td>qwen1.5-110b_4</td><td>Qwen1.5-110B</td><td>openai-text</td></tr><tr><td>yi1.5-34b</td><td>Yi1.5-34B</td><td>openai-text</td></tr><tr><td>qwen2-72b_1</td><td>Qwen2-72B</td><td>openai-text</td></tr><tr><td>qwen2-72b_4</td><td>Qwen2-72B</td><td>openai-text</td></tr><tr><td>falcon2-11b</td><td>Falcon2-11B</td><td>openai-text</td></tr><tr><td>openbiollm-llama3-70b_1</td><td>OpenBioLLM-Llama3-70B</td><td>openai-text</td></tr><tr><td>openbiollm-llama3-70b_4</td><td>OpenBioLLM-Llama3-70B</td><td>openai-text</td></tr><tr><td>minicpm-v2.5</td><td>MiniCPM-Llama3-V2.5</td><td>ollama-text</td></tr><tr><td>codestral-22b-v0.1</td><td>Codestral-22B-v0.1</td><td>openai-text</td></tr><tr><td>gemma2-27b_4</td><td>Gemma-2-27B</td><td>openai-text</td></tr><tr><td>llama3.1-405b_8</td><td>Llama-3.1-405B</td><td>openai-text</td></tr><tr><td>mistral-123b_4</td><td>Mistral-123B</td><td>openai-text</td></tr><tr><td>flux.1-dev</td><td>FLUX.1-dev</td><td>openai-image</td></tr><tr><td>nemotron-70b_2</td><td>Llama-3.1-Nemotron-70B</td><td>openai-text</td></tr><tr><td>nvlm-70b_4</td><td>NVLM-D-72B</td><td>openai-text</td></tr></tbody></table><h1 id="docker-이미지-구축-방법" tabindex="-1"><a class="header-anchor" href="#docker-이미지-구축-방법" aria-hidden="true">#</a> Docker 이미지 구축 방법</h1><p>다음은 모델 서비스 이미지를 구축하는 방법에 대한 개요입니다:</p><h2 id="_1-전달-요구-사항-명확화" tabindex="-1"><a class="header-anchor" href="#_1-전달-요구-사항-명확화" aria-hidden="true">#</a> 1. 전달 요구 사항 명확화</h2><p>이미지를 구축하기 전에, 배포 이미지가 요구 목표에 부합하는지 확인하기 위해 요구 사항을 명확히 해야 합니다. 주요 요소는 다음과 같습니다:</p><ul><li><p><strong>모델 선택</strong>: 배포할 모델 유형을 결정합니다. 예를 들어 텍스트 생성, 이미지 생성, 또는 멀티모달 모델 등을 선택합니다. 구체적인 모델은 Llama3-8B, Qwen1.5-110B, Falcon2-11B 중 선택할 수 있습니다.</p></li><li><p><strong>배포 자원</strong>: 배포할 하드웨어 자원을 명확히 하고, 특히 GPU 가속이 필요한지 여부와 사용할 GPU 유형(NVIDIA A100, H100, RTX4090 등)과 수량을 결정합니다.</p></li><li><p><strong>배포 방식</strong>: 로컬 배포인지 클라우드 배포인지, 모델 파일을 이미지에 포함시킬지 등을 결정합니다.</p></li><li><p><strong>서비스 인터페이스 프로토콜</strong>: 모델 서비스가 표준화된 방식으로 액세스될 수 있도록 적절한 API 프로토콜을 선택합니다. 다양한 모델 서비스 유형에 맞는 프로토콜이 제공됩니다. 예를 들어 텍스트 생성에는 OpenAI 프로토콜, Ollama 프로토콜, HF의 TGI 프로토콜 등이 있습니다.</p></li><li><p><strong>성능 매개 변수</strong>: 모델의 성능 목표를 명확히 합니다. 예를 들어 처리량, 응답 시간, 확장성 요구 사항 등이 포함됩니다.</p></li></ul><h2 id="_2-요구-사항에-가장-부합하는-오픈-소스-도구-프레임워크-선택" tabindex="-1"><a class="header-anchor" href="#_2-요구-사항에-가장-부합하는-오픈-소스-도구-프레임워크-선택" aria-hidden="true">#</a> 2. 요구 사항에 가장 부합하는 오픈 소스 도구/프레임워크 선택</h2><p>모델의 유형과 배포 환경에 따라, 이미지 구축 작업을 간소화하기 위해 적합한 오픈 소스 도구와 프레임워크를 선택합니다. 작업 유형에 따라 일반적인 도구/프레임워크는 다음과 같습니다:</p><ul><li><p><strong>텍스트 생성</strong>: lmdeploy, Ollama, vllm, localai, TGI</p></li><li><p><strong>이미지 생성</strong>: ComfyUI, Stable Diffusion WebUI, localai</p></li></ul><h2 id="_3-개발-및-디버깅" tabindex="-1"><a class="header-anchor" href="#_3-개발-및-디버깅" aria-hidden="true">#</a> 3. 개발 및 디버깅</h2><p>이미지를 구축하기 전에, 모델이 컨테이너에서 정상적으로 작동하는지 확인하기 위해 개발 및 디버깅 과정을 거쳐야 합니다. 주요 단계는 다음과 같습니다:</p><ul><li><p><strong>배포 매개 변수 조정</strong>: 작업 요구 사항 및 모델 매개 변수에 따라, Docker Compose 파일의 실행 매개 변수와 환경 변수를 조정하여 모델이 올바르게 작동하도록 합니다.</p></li><li><p><strong>모델 적응</strong>: 모델이 컨테이너 환경에서 원활하게 실행될 수 있도록 보장하며, 종속성 설치 및 구성 문제를 처리합니다. 특정 모델은 특정 라이브러리, 드라이버 또는 환경 변수 설정을 필요로 할 수 있습니다. Dockerfile에서 이러한 종속성을 정확하게 정의합니다. 일부 모델은 새로운 모델 구조를 도입할 수 있으며, 이러한 경우 추가적인 코드 수정이 필요할 수 있습니다.</p></li><li><p><strong>버그 해결</strong>: 이미지 실행 중 발생하는 오류를 디버깅하고, 오픈 소스 프로젝트에 존재할 수 있는 버그를 해결합니다.</p></li><li><p><strong>코드 최적화</strong>: 모델 테스트 실행 중 발견된 성능 문제를 최적화합니다. 예를 들어 자원 할당, 모델 스케줄링 등 논리를 최적화하여 실행 성능을 개선합니다.</p></li></ul><h2 id="_4-이미지-빌드" tabindex="-1"><a class="header-anchor" href="#_4-이미지-빌드" aria-hidden="true">#</a> 4. 이미지 빌드</h2><p>개발과 디버깅이 완료되면 Docker 이미지를 빌드할 수 있습니다. 구체적인 단계는 다음과 같습니다:</p><ol><li><p><strong>Dockerfile 작성</strong>: 상황에 따라 Dockerfile 작성 방법은 다음과 같은 방식이 있을 수 있습니다:</p><ul><li>오픈 소스 프로젝트 Dockerfile을 기반으로 수정 후 다시 빌드</li><li>오픈 소스 이미지를 기본 이미지로 사용하여 수정된 내용을 추가 후 증분 빌드</li><li>Dockerfile을 처음부터 작성하여 새로운 맞춤형 이미지를 빌드</li></ul></li><li><p><strong>이미지 빌드</strong>:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> build <span class="token parameter variable">-t</span> <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span> <span class="token builtin class-name">.</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><p><strong>이미지 테스트</strong>: GPU 자원을 사용하여 컨테이너를 실행하고 테스트하여 이미지의 모델 서비스가 요청에 정상적으로 응답하는지 확인합니다.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code> <span class="token function">docker</span> run <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-d</span> <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><p><strong>이미지 레지스트리에 푸시</strong>: 다중 노드 또는 클라우드 환경에서 배포하려면 Docker Hub, AWS ECR 등 컨테이너 이미지 레지스트리에 이미지를 푸시할 수 있습니다.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> push <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>위의 단계를 통해 모델 서비스를 위한 Docker 이미지 구축을 완료할 수 있으며, 요구 사항을 충족하는 모델이 올바르게 작동하도록 할 수 있습니다.</p></li></ol>`,27))])}const b=s(p,[["render",h],["__file","docker_build.html.vue"]]);export{b as default};
