import{_ as o,H as s,W as r,X as i,Y as t,Z as a,$ as d,a0 as l}from"./framework-cd0f4961.js";const c={},p={href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html",target:"_blank",rel:"noopener noreferrer"};function h(m,e){const n=s("ExternalLinkIcon");return r(),i("div",null,[e[2]||(e[2]=t("h1",{id:"how-to-use",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#how-to-use","aria-hidden":"true"},"#"),a(" How to use")],-1)),e[3]||(e[3]=t("h2",{id:"prerequisites",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#prerequisites","aria-hidden":"true"},"#"),a(" Prerequisites")],-1)),t("p",null,[e[1]||(e[1]=a("install nvidia-container-toolkit, see: ")),t("a",p,[e[0]||(e[0]=a("https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html")),d(n)])]),e[4]||(e[4]=l(`<h2 id="start-the-service" tabindex="-1"><a class="header-anchor" href="#start-the-service" aria-hidden="true">#</a> Start the service</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./start-service.sh <span class="token operator">&lt;</span>config-name<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="request-the-service" tabindex="-1"><a class="header-anchor" href="#request-the-service" aria-hidden="true">#</a> Request the service</h2><h3 id="openai-text-api" tabindex="-1"><a class="header-anchor" href="#openai-text-api" aria-hidden="true">#</a> openai-text api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-openai-text.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="ollama-text-api" tabindex="-1"><a class="header-anchor" href="#ollama-text-api" aria-hidden="true">#</a> ollama-text api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-ollama-text.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="openai-image-api" tabindex="-1"><a class="header-anchor" href="#openai-image-api" aria-hidden="true">#</a> openai-image api</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./request-service-openai-image.sh <span class="token operator">&lt;</span>model<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="stop-the-service" tabindex="-1"><a class="header-anchor" href="#stop-the-service" aria-hidden="true">#</a> Stop the service</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> ./stop-service.sh <span class="token operator">&lt;</span>config-name<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="table-config-name-model-api-protocol" tabindex="-1"><a class="header-anchor" href="#table-config-name-model-api-protocol" aria-hidden="true">#</a> Table: config-name -&gt; model &amp; api-protocol</h2><table><thead><tr><th>config-name</th><th>model</th><th>api-protocol</th></tr></thead><tbody><tr><td>llama3-8b</td><td>Llama3-8B</td><td>openai-text</td></tr><tr><td>llama3-70b_1</td><td>Llama3-70B</td><td>openai-text</td></tr><tr><td>llama3-70b_4</td><td>Llama3-70B</td><td>openai-text</td></tr><tr><td>qwen1.5-110b_1</td><td>Qwen1.5-110B</td><td>openai-text</td></tr><tr><td>qwen1.5-110b_4</td><td>Qwen1.5-110B</td><td>openai-text</td></tr><tr><td>yi1.5-34b</td><td>Yi1.5-34B</td><td>openai-text</td></tr><tr><td>qwen2-72b_1</td><td>Qwen2-72B</td><td>openai-text</td></tr><tr><td>qwen2-72b_4</td><td>Qwen2-72B</td><td>openai-text</td></tr><tr><td>falcon2-11b</td><td>Falcon2-11B</td><td>openai-text</td></tr><tr><td>openbiollm-llama3-70b_1</td><td>OpenBioLLM-Llama3-70B</td><td>openai-text</td></tr><tr><td>openbiollm-llama3-70b_4</td><td>OpenBioLLM-Llama3-70B</td><td>openai-text</td></tr><tr><td>minicpm-v2.5</td><td>MiniCPM-Llama3-V2.5</td><td>ollama-text</td></tr><tr><td>codestral-22b-v0.1</td><td>Codestral-22B-v0.1</td><td>openai-text</td></tr><tr><td>gemma2-27b_4</td><td>Gemma-2-27B</td><td>openai-text</td></tr><tr><td>llama3.1-405b_8</td><td>Llama-3.1-405B</td><td>openai-text</td></tr><tr><td>mistral-123b_4</td><td>Mistral-123B</td><td>openai-text</td></tr><tr><td>flux.1-dev</td><td>FLUX.1-dev</td><td>openai-image</td></tr><tr><td>nemotron-70b_2</td><td>Llama-3.1-Nemotron-70B</td><td>openai-text</td></tr><tr><td>nvlm-70b_4</td><td>NVLM-D-72B</td><td>openai-text</td></tr></tbody></table><h1 id="how-to-build-a-docker-image" tabindex="-1"><a class="header-anchor" href="#how-to-build-a-docker-image" aria-hidden="true">#</a> How to build a docker image</h1><p>Here is an overview of how to build a model service image:</p><h2 id="_1-clarify-delivery-requirements" tabindex="-1"><a class="header-anchor" href="#_1-clarify-delivery-requirements" aria-hidden="true">#</a> 1. Clarify delivery requirements</h2><p>Before building an image, you first need to clarify the delivery requirements to ensure that the deployment image is consistent with the required goals. Here are some key elements:</p><ul><li><p><strong>Model selection</strong>: Determine the type of model to be deployed, such as text, text, or multimodal. Determine the specific model, such as Llama3-8B, Qwen1.5-110B, or Falcon2-11B.</p></li><li><p><strong>Deployment resources</strong>: Clarify the hardware resources that will be used for deployment, especially whether GPU acceleration is required, and the type of GPU selected (such as NVIDIA A100, H100, RTX4090, etc.) and quantity.</p></li><li><p><strong>Deployment method</strong>: Determine the deployment method, whether it is local deployment or cloud deployment, whether the model file needs to be packaged into the image, etc.</p></li><li><p><strong>Service interface protocol</strong>: Select the appropriate API protocol to ensure that the model service can be accessed in a standardized way. Different types of model services have different protocols to choose from. For example, common protocols for Wenshengwen include OpenAI protocol, Ollama protocol, HF&#39;s TGI protocol, etc. &gt;.</p></li><li><p><strong>Performance parameters</strong>: clarify some performance goals of the model, such as throughput, response time, scalability requirements, etc.</p></li></ul><h2 id="_2-find-the-open-source-tools-frameworks-that-best-meet-your-needs" tabindex="-1"><a class="header-anchor" href="#_2-find-the-open-source-tools-frameworks-that-best-meet-your-needs" aria-hidden="true">#</a> 2. Find the open source tools/frameworks that best meet your needs</h2><p>According to the type of model and deployment environment, choose the appropriate open source tools and frameworks to simplify the construction work. According to different types of tasks, common tool frameworks include:</p><ul><li><p><strong>Wenshengwen</strong>: lmdeploy, Ollama, VLLM, Localai, TGI</p></li><li><p><strong>Wenshengtu</strong>: comfyui, stable-diffusion-webui, localai</p></li></ul><h2 id="_3-development-and-debugging" tabindex="-1"><a class="header-anchor" href="#_3-development-and-debugging" aria-hidden="true">#</a> 3. Development and debugging</h2><p>Before building the image, development and debugging are required to ensure that the model can run normally in the container. The steps of development and debugging include:</p><ul><li><p><strong>Adjust deployment parameters</strong>: According to task requirements, model parameters, etc., adjust the running parameters, environment variables, etc. in the docker compose file to ensure the correct operation of the model.</p></li><li><p><strong>Model adaptation</strong>: Ensure that the model can run seamlessly in the container environment and handle the installation and configuration of dependencies. Some models may require specific libraries, drivers, or environment variable settings. Define these dependencies accurately in the Dockerfile. Some models may introduce new model structures, which require further customized code development to adapt.</p></li><li><p><strong>Solve bugs</strong>: Troubleshoot errors reported during the test image runtime to resolve possible bugs in open source projects.</p></li><li><p><strong>Optimize code implementation</strong>: Optimize code implementation such as resource allocation, model scheduling, and other logic for some performance issues found during model testing to ensure reasonable performance.</p></li></ul><h2 id="_4-image-packaging" tabindex="-1"><a class="header-anchor" href="#_4-image-packaging" aria-hidden="true">#</a> 4. Image packaging</h2><p>After completing development and debugging, you can start packaging the Docker image. Here are the specific steps:</p><ol><li><strong>Write Dockerfile</strong>: Depending on the situation, there are the following possible ways to write Dockerfile:</li></ol><ul><li><p>Based on the open source project Dockerfile, modify it and rebuild it from scratch</p></li><li><p>Based on the open source image as the base image, add the modified content and perform incremental build</p></li><li><p>Write your own Dockerfile and build a new customized image from scratch</p></li></ul><ol start="2"><li><strong>Build the image</strong>:</li></ol><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> build <span class="token parameter variable">-t</span> <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span> <span class="token builtin class-name">.</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol start="3"><li><strong>Test the image</strong>: Use GPU resources to start the container and test it to ensure that the model service in the image can respond to requests normally.</li></ol><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-d</span> <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol start="4"><li><strong>Push to the image repository</strong>: If you want to deploy in a multi-node or cloud environment, you can push the image to a container image repository such as Docker Hub, AWS ECR, etc.:</li></ol><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> push <span class="token operator">&lt;</span>your-image-name<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Through the above steps, you can successfully complete the construction of a Docker image for model service, ensuring that the model can run correctly and meet the requirements.</p>`,35))])}const g=o(c,[["render",h],["__file","docker_build.html.vue"]]);export{g as default};
